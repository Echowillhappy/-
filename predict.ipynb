{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69e4e7d-c303-4a93-8163-4cbcdf55cf51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/miniconda3/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded fine-tuned model from ./results_quad_extraction/final_lora_adapter\n",
      "Generating predictions for 2000 items from test1.json with batch_size=32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Predictions: 100%|██████████| 63/63 [36:06<00:00, 34.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All predictions for test1.json saved to ./results_quad_extraction/test1_predictions.txt\n",
      "Each line in the output file is a JSON object containing 'id', 'original_content', and 'predicted_output'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm # For progress bar\n",
    "\n",
    "import config\n",
    "from optimized_prompt_template import OPTIMIZED_PROMPT_SYSTEM_MESSAGE, OPTIMIZED_PROMPT_CORE_INSTRUCTIONS, FEW_SHOT_EXAMPLES_TEXT\n",
    "\n",
    "# --- 修改 generate_prediction_for_test 为 generate_prediction_for_batch ---\n",
    "# 接受一个文本列表，返回一个预测结果列表\n",
    "def generate_prediction_for_batch(model, tokenizer, comment_texts):\n",
    "    # 构造用户消息列表\n",
    "    user_message_contents = []\n",
    "    for text in comment_texts:\n",
    "        user_message_contents.append(f\"{OPTIMIZED_PROMPT_CORE_INSTRUCTIONS}\\n\\n{FEW_SHOT_EXAMPLES_TEXT}\\n\\n[待处理文本]\\n{text}\")\n",
    "\n",
    "    # 应用聊天模板，对整个批次进行分词\n",
    "    # padding=True 会将批次中的所有序列填充到最长序列的长度\n",
    "    # truncation=True 确保不会超过模型的最大长度\n",
    "    prompt_for_models = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": OPTIMIZED_PROMPT_SYSTEM_MESSAGE},\n",
    "                {\"role\": \"user\", \"content\": content_text}\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ) for content_text in user_message_contents\n",
    "    ]\n",
    "\n",
    "    # 对整个批次进行分词\n",
    "    # padding=True 是批量推理的关键，它会将所有序列填充到批次中最长的序列长度\n",
    "    # truncation=True 是为了避免过长的输入导致OOM，需要结合 config.MAX_SEQ_LENGTH\n",
    "    inputs = tokenizer(\n",
    "        prompt_for_models,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True, # <-- 关键改变：启用填充\n",
    "        truncation=True, # <-- 建议添加：启用截断\n",
    "        max_length=getattr(config, 'MAX_SEQ_LENGTH', 2048) # <-- 建议在 config 中定义，否则使用默认值\n",
    "    ).to(model.device)\n",
    "\n",
    "    # 记录原始输入的长度，用于后续解码时截断模型生成的响应\n",
    "    # 注意：对于批量，所有输入的长度都因为 padding=True 而变得一样\n",
    "    input_ids_len = inputs.input_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=512,\n",
    "            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "\n",
    "    # 处理批次生成的输出\n",
    "    all_assistant_responses = []\n",
    "    # outputs 的形状是 [batch_size, generated_sequence_length]\n",
    "    for i in range(len(comment_texts)):\n",
    "        # 解码时从原始输入长度之后开始，以获取助手的回复部分\n",
    "        generated_ids_for_sample = outputs[i][input_ids_len:]\n",
    "        assistant_response = tokenizer.decode(generated_ids_for_sample, skip_special_tokens=True).strip()\n",
    "\n",
    "        # 后处理\n",
    "        if not assistant_response:\n",
    "            assistant_response = \"NULL | NULL | non-hate | non-hate [END]\"\n",
    "        elif not assistant_response.strip().endswith(\"[END]\"):\n",
    "            assistant_response = assistant_response.strip() + \" [END]\" # Basic cleanup\n",
    "\n",
    "        all_assistant_responses.append(assistant_response)\n",
    "\n",
    "    return all_assistant_responses\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    adapter_path = os.path.join(config.OUTPUT_DIR, \"final_lora_adapter\")\n",
    "    if not os.path.exists(adapter_path):\n",
    "        raise FileNotFoundError(f\"Fine-tuned adapter not found at {adapter_path}. Please run train.py first.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        # tokenizer.pad_token_id = tokenizer.eos_token_id # 确保 pad_token_id 在此设置后正确\n",
    "\n",
    "    compute_dtype_inf = getattr(torch, config.BNB_4BIT_COMPUTE_DTYPE)\n",
    "    bnb_config_inf = None\n",
    "    if config.USE_4BIT_QUANTIZATION: # Optional: Quantize for inference too\n",
    "        bnb_config_inf = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=config.BNB_4BIT_QUANT_TYPE,\n",
    "            bnb_4bit_compute_dtype=compute_dtype_inf,\n",
    "        )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.BASE_MODEL_ID,\n",
    "        quantization_config=bnb_config_inf,\n",
    "        torch_dtype=compute_dtype_inf if bnb_config_inf else torch.bfloat16,\n",
    "        device_map={\"\": 0}, # 强制加载到 GPU 0，确保显存够用\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    # model = model.merge_and_unload() # 可选：如果显存足够且不再训练，可以取消注释以提高微小速度\n",
    "    model.eval() # Set to evaluation mode\n",
    "    print(f\"Loaded fine-tuned model from {adapter_path}\")\n",
    "\n",
    "    # --- Load Test Data (e.g., test1.json) ---\n",
    "    if not os.path.exists(config.TEST_INPUT_FILE):\n",
    "        print(f\"Test input file {config.TEST_INPUT_FILE} not found. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    with open(config.TEST_INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        test_data = json.load(f) # Expecting list of {\"id\": ..., \"content\": ...}\n",
    "\n",
    "    # --- 设置批次大小 ---\n",
    "    # 这是关键参数，根据你的 GPU 显存和模型大小进行调整\n",
    "    # 比如 24GB GPU + 7B 4bit 模型，可以尝试 4-8，甚至更大如果显存允许\n",
    "    BATCH_SIZE = 32 # 示例值，请务必根据你的硬件情况调整！\n",
    "\n",
    "    all_predictions_output = []\n",
    "    print(f\"Generating predictions for {len(test_data)} items from {config.TEST_INPUT_FILE} with batch_size={BATCH_SIZE}...\")\n",
    "\n",
    "    # --- 批量生成预测 ---\n",
    "    # 使用 tqdm 包装批次循环，显示整体进度\n",
    "    for i in tqdm(range(0, len(test_data), BATCH_SIZE), desc=\"Generating Predictions\"):\n",
    "        # 获取当前批次的样本\n",
    "        batch_items = test_data[i:i + BATCH_SIZE]\n",
    "        batch_comment_texts = [item[\"content\"] for item in batch_items]\n",
    "        batch_ids = [item.get(\"id\", \"N/A\") for item in batch_items]\n",
    "\n",
    "        # 调用批量生成函数\n",
    "        batch_prediction_strings = generate_prediction_for_batch(model, tokenizer, batch_comment_texts)\n",
    "\n",
    "        # 收集每个样本的预测结果\n",
    "        for j, pred_str in enumerate(batch_prediction_strings):\n",
    "            all_predictions_output.append({\n",
    "                \"id\": batch_ids[j],\n",
    "                \"predicted_output\": pred_str,\n",
    "                \"original_content\": batch_comment_texts[j]\n",
    "            })\n",
    "\n",
    "    # --- 保存预测结果 ---\n",
    "    os.makedirs(os.path.dirname(config.TEST_PREDICTIONS_FILE), exist_ok=True)\n",
    "\n",
    "    # 保存为 JSONL 文件 (每行一个 JSON 对象)\n",
    "    with open(config.TEST_PREDICTIONS_FILE, 'w', encoding='utf-8') as f:\n",
    "        for pred_obj in all_predictions_output:\n",
    "            f.write(json.dumps(pred_obj, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"\\nAll predictions for {config.TEST_INPUT_FILE} saved to {config.TEST_PREDICTIONS_FILE}\")\n",
    "    print(f\"Each line in the output file is a JSON object containing 'id', 'original_content', and 'predicted_output'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54fc3103-ef87-4803-a757-17972bb0b02b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/miniconda3/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mUSE_4BIT_QUANTIZATION:\n\u001b[1;32m     75\u001b[0m     bnb_config_inf \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     76\u001b[0m         load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     77\u001b[0m         bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mBNB_4BIT_QUANT_TYPE,\n\u001b[1;32m     78\u001b[0m         bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype_inf,\n\u001b[1;32m     79\u001b[0m     )\n\u001b[0;32m---> 81\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASE_MODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_dtype_inf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbnb_config_inf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model, adapter_path)\n\u001b[1;32m     90\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py:3657\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3654\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3657\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\n\u001b[1;32m   3659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3660\u001b[0m     torch_dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_torch_dtype(torch_dtype)\n\u001b[1;32m   3661\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:82\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n\u001b[1;32m     81\u001b[0m bnb_multibackend_is_enabled \u001b[38;5;241m=\u001b[39m is_bitsandbytes_multi_backend_available()\n\u001b[0;32m---> 82\u001b[0m \u001b[43mvalidate_bnb_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_flax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting into 4-bit or 8-bit weights from tf/flax weights is currently not supported, please make\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m sure the weights are in PyTorch format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/integrations/bitsandbytes.py:558\u001b[0m, in \u001b[0;36mvalidate_bnb_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_multi_backend_available():\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _validate_bnb_multi_backend_availability(raise_exception)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_validate_bnb_cuda_backend_availability\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraise_exception\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/integrations/bitsandbytes.py:536\u001b[0m, in \u001b[0;36m_validate_bnb_cuda_backend_availability\u001b[0;34m(raise_exception)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exception:\n\u001b[1;32m    535\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(log_msg)\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(log_msg)\n\u001b[1;32m    538\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(log_msg)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA is required but not available for bitsandbytes. Please consider installing the multi-platform enabled version of bitsandbytes, which is currently a work in progress. Please check currently supported platforms and installation instructions at https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "import json\n",
    "import os\n",
    "\n",
    "import config\n",
    "from optimized_prompt_template import OPTIMIZED_PROMPT_CORE_INSTRUCTIONS\n",
    "\n",
    "def generate_prediction_for_batch(model, tokenizer, comment_texts):\n",
    "    # 构造用户提示，去掉 few-shot，仅保留核心 instructions\n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": OPTIMIZED_PROMPT_CORE_INSTRUCTIONS},\n",
    "                {\"role\": \"user\", \"content\": f'\\n\\n输入: \"{text}\"\\n输出:'}\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        ) for text in comment_texts\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=getattr(config, 'MAX_SEQ_LENGTH', 2048)\n",
    "    ).to(model.device)\n",
    "\n",
    "    input_ids_len = inputs.input_ids.shape[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            attention_mask=inputs.attention_mask,\n",
    "            max_new_tokens=256,\n",
    "            pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "    results = []\n",
    "    for i in range(len(comment_texts)):\n",
    "        generated_ids = outputs[i][input_ids_len:]\n",
    "        decoded = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        # 只保留四元组部分，去掉任何模型思考过程\n",
    "        if \"输出:\" in decoded:\n",
    "            prediction = decoded.split(\"输出:\")[-1].strip()\n",
    "        else:\n",
    "            prediction = decoded.strip()\n",
    "\n",
    "        # 补充 [END]，防止未生成\n",
    "        if not prediction.endswith(\"[END]\"):\n",
    "            prediction += \" [END]\"\n",
    "\n",
    "        results.append(prediction)\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    adapter_path = os.path.join(config.OUTPUT_DIR, \"final_lora_adapter\")\n",
    "    if not os.path.exists(adapter_path):\n",
    "        raise FileNotFoundError(f\"Fine-tuned adapter not found at {adapter_path}. Please run train.py first.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(adapter_path, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    compute_dtype_inf = getattr(torch, config.BNB_4BIT_COMPUTE_DTYPE)\n",
    "    bnb_config_inf = None\n",
    "    if config.USE_4BIT_QUANTIZATION:\n",
    "        bnb_config_inf = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=config.BNB_4BIT_QUANT_TYPE,\n",
    "            bnb_4bit_compute_dtype=compute_dtype_inf,\n",
    "        )\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.BASE_MODEL_ID,\n",
    "        quantization_config=bnb_config_inf,\n",
    "        torch_dtype=compute_dtype_inf if bnb_config_inf else torch.bfloat16,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    model.eval()\n",
    "    print(f\"✅ 模型已加载：{adapter_path}\")\n",
    "\n",
    "    # 加载测试数据，仅取前5条\n",
    "    if not os.path.exists(config.TEST_INPUT_FILE):\n",
    "        raise FileNotFoundError(f\"Test input file {config.TEST_INPUT_FILE} not found.\")\n",
    "\n",
    "    with open(config.TEST_INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    test_data = raw_data[:5]\n",
    "\n",
    "    comment_texts = [item[\"content\"] for item in test_data]\n",
    "    predictions = generate_prediction_for_batch(model, tokenizer, comment_texts)\n",
    "\n",
    "    print(\"\\n=== 模型输出结果（仅四元组） ===\")\n",
    "    for result in predictions:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b7449-f776-42b0-b6f9-cab4ccd11503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
