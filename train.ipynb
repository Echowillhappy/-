{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c22188-a129-4e79-8194-2113e273847d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###train.py显示进度版本↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2430a273-46f4-408e-80a6-60eaca8c5c49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/miniconda3/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset with ratio: 0.1\n",
      "Training samples: 3600, Validation samples: 400\n",
      "Applying formatting_func to training dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbfd03778ae4a61bcaa2663addf683d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting train dataset:   0%|          | 0/3600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying formatting_func to evaluation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3d01d5755d42bd9ec78a76dc009396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting eval dataset:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,784,704 || all params: 467,772,416 || trainable%: 0.8091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/miniconda3/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/miniconda3/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting SFT training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747289e3f204406b9f0a5ea62f90668c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Steps:   0%|          | 0/900 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 12:09, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.383800</td>\n",
       "      <td>0.377134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.394000</td>\n",
       "      <td>0.370568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen1.5-0.5B-Chat/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f81044891f0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 47d7024c-0dff-4fde-85c5-4d2d593f1606)') - silently ignoring the lookup for the file config.json in Qwen/Qwen1.5-0.5B-Chat.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in Qwen/Qwen1.5-0.5B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen1.5-0.5B-Chat/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f81043d6d30>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 506ce2be-dc3f-4d90-9f95-0f4b4dca1d6d)') - silently ignoring the lookup for the file config.json in Qwen/Qwen1.5-0.5B-Chat.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in Qwen/Qwen1.5-0.5B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/other.py:689: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen1.5-0.5B-Chat/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f8104292370>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 37af9a89-595b-4692-a973-98172d9b8d0d)') - silently ignoring the lookup for the file config.json in Qwen/Qwen1.5-0.5B-Chat.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in Qwen/Qwen1.5-0.5B-Chat - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final LoRA adapter and tokenizer saved to: ./results_quad_extraction614/final_lora_adapter\n",
      "Training/validation loss plot saved to ./results_quad_extraction614/training_plots/training_validation_loss.png\n",
      "\n",
      "--- Evaluating on Validation Split Post-Training ---\n",
      "\n",
      "Generating predictions for 400 validation samples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d256155aa3e477ebdb92ce224cfeaf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Prediction:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation split predictions saved to ./results_quad_extraction614/validation_split_predictions.txt\n",
      "\n",
      "Validation Set Evaluation Results:\n",
      "  Hard Match -> F1: 0.0493, P: 0.0493, R: 0.0494\n",
      "  Soft Match -> F1: 0.1672, P: 0.1670, R: 0.1674\n",
      "  Overall Score (Avg F1): 0.1083\n",
      "Validation evaluation F1 scores plot saved to ./results_quad_extraction614/validation_evaluation_plots/validation_set_f1_scores.png\n",
      "\n",
      "Training and validation evaluation (if applicable) complete.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl\n",
    ")\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import from our project\n",
    "import config\n",
    "from optimized_prompt_template import OPTIMIZED_PROMPT_SYSTEM_MESSAGE, OPTIMIZED_PROMPT_CORE_INSTRUCTIONS, FEW_SHOT_EXAMPLES_TEXT\n",
    "# Import evaluation utilities\n",
    "from evaluate import parse_output_line, calculate_f1_metrics_from_lists, plot_evaluation_scores\n",
    "\n",
    "# --- Global Variables for Callbacks and Plotting ---\n",
    "tokenizer = None\n",
    "train_pbar = None # For TqdmProgressCallback\n",
    "\n",
    "# --- Tqdm Progress Callback for SFTTrainer ---\n",
    "class TqdmProgressCallback(TrainerCallback):\n",
    "    def on_train_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        global train_pbar\n",
    "        if state.is_world_process_zero: # Only on the main process\n",
    "            total_steps = state.max_steps\n",
    "            train_pbar = tqdm(total=total_steps, desc=\"Training Steps\", unit=\"step\")\n",
    "\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        global train_pbar\n",
    "        if state.is_world_process_zero and train_pbar is not None:\n",
    "            train_pbar.update(1)\n",
    "            # Optionally, update postfix with current loss/epoch\n",
    "            logs = kwargs.get(\"logs\", None)\n",
    "            if logs and 'loss' in logs:\n",
    "                current_loss = logs['loss']\n",
    "                current_epoch = state.epoch\n",
    "                train_pbar.set_postfix_str(f\"Loss: {current_loss:.4f}, Epoch: {current_epoch:.2f}\")\n",
    "\n",
    "\n",
    "    def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        global train_pbar\n",
    "        if state.is_world_process_zero and train_pbar is not None:\n",
    "            train_pbar.close()\n",
    "            train_pbar = None\n",
    "\n",
    "    # Optional: If you want the progress bar to pause during evaluation\n",
    "    def on_evaluate_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        global train_pbar\n",
    "        if state.is_world_process_zero and train_pbar is not None:\n",
    "            train_pbar.refresh() # Ensure it's updated before pausing (if needed)\n",
    "            train_pbar.set_postfix_str(\"Evaluating...\") # Set a temporary postfix\n",
    "\n",
    "    def on_evaluate_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        global train_pbar\n",
    "        if state.is_world_process_zero and train_pbar is not None:\n",
    "            train_pbar.refresh() # Ensure it's updated after resuming\n",
    "            # The on_step_end will automatically update it with loss again\n",
    "            # Or you can explicitly reset it if no step happens immediately after eval\n",
    "            if state.global_step < state.max_steps: # Only reset if training is not yet finished\n",
    "                 logs = kwargs.get(\"logs\", None)\n",
    "                 if logs and 'loss' in logs:\n",
    "                    current_loss = logs['loss']\n",
    "                    current_epoch = state.epoch\n",
    "                    train_pbar.set_postfix_str(f\"Loss: {current_loss:.4f}, Epoch: {current_epoch:.2f}\")\n",
    "                 else: # If no loss log, just clear it or set a generic training message\n",
    "                    train_pbar.set_postfix_str(\"Training resumed...\")\n",
    "\n",
    "\n",
    "def plot_training_history(log_history, output_dir, train_dataset_size, train_batch_size, gradient_accumulation_steps):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    epochs_train = []\n",
    "    losses_train = []\n",
    "    epochs_eval = []\n",
    "    losses_eval = []\n",
    "    \n",
    "    steps_per_epoch = None\n",
    "    if train_dataset_size > 0 and train_batch_size > 0 and gradient_accumulation_steps > 0:\n",
    "        effective_batch_size = train_batch_size * gradient_accumulation_steps\n",
    "        if effective_batch_size > 0:\n",
    "            steps_per_epoch = (train_dataset_size + effective_batch_size - 1) // effective_batch_size # Ceiling division\n",
    "            if steps_per_epoch == 0: steps_per_epoch = 1\n",
    "\n",
    "    for log_entry in log_history:\n",
    "        current_epoch_val = log_entry.get('epoch')\n",
    "        current_step = log_entry.get('step')\n",
    "\n",
    "        if 'loss' in log_entry: # Training loss\n",
    "            if current_epoch_val is not None:\n",
    "                epochs_train.append(current_epoch_val)\n",
    "            elif current_step is not None and steps_per_epoch is not None:\n",
    "                epochs_train.append(current_step / steps_per_epoch)\n",
    "            else:\n",
    "                epochs_train.append(current_step if current_step is not None else float(len(epochs_train) + 1)) # Use a sensible default\n",
    "\n",
    "            losses_train.append(log_entry['loss'])\n",
    "\n",
    "        if 'eval_loss' in log_entry: # Evaluation loss\n",
    "            if current_epoch_val is not None:\n",
    "                epochs_eval.append(current_epoch_val)\n",
    "            elif current_step is not None and steps_per_epoch is not None:\n",
    "                epochs_eval.append(current_step / steps_per_epoch)\n",
    "            else:\n",
    "                epochs_eval.append(epochs_train[-1] if epochs_train else float(len(epochs_eval) + 1)) # Use a sensible default\n",
    "            losses_eval.append(log_entry['eval_loss'])\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    if epochs_train and losses_train:\n",
    "        plt.plot(epochs_train, losses_train, label='Training Loss', marker='.', linestyle='-')\n",
    "    if epochs_eval and losses_eval:\n",
    "        plt.plot(epochs_eval, losses_eval, label='Validation Loss', marker='o', linestyle='--')\n",
    "    \n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plot_path = os.path.join(output_dir, \"training_validation_loss.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"Training/validation loss plot saved to {plot_path}\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# MODIFIED: sft_formatting_func now returns tokenized input_ids and labels\n",
    "def sft_formatting_func(example): \n",
    "    # 'example' here will be a dictionary like {'id': ..., 'content': ..., 'output': ...}\n",
    "    comment_to_process = example['content']\n",
    "    target_quadruplet = example['output'] \n",
    "    \n",
    "    user_message_content = f\"{OPTIMIZED_PROMPT_CORE_INSTRUCTIONS}\\n\\n{FEW_SHOT_EXAMPLES_TEXT}\\n\\n[待处理文本]\\n{comment_to_process}\"\n",
    "    \n",
    "    dialogue = [\n",
    "        {\"role\": \"system\", \"content\": OPTIMIZED_PROMPT_SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": user_message_content},\n",
    "        {\"role\": \"assistant\", \"content\": target_quadruplet}\n",
    "    ]\n",
    "    \n",
    "    # Ensure tokenizer is initialized and accessible\n",
    "    if tokenizer is None:\n",
    "        raise ValueError(\"Tokenizer is not initialized globally for sft_formatting_func.\")\n",
    "    \n",
    "    # Apply chat template to get the full string\n",
    "    full_text = tokenizer.apply_chat_template(dialogue, tokenize=False, add_generation_prompt=False)\n",
    "    \n",
    "    # Tokenize the full string\n",
    "    tokenized_output = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\", # Pad to max_seq_length\n",
    "        max_length=config.MAX_SEQ_LENGTH,\n",
    "        return_tensors=\"pt\" # Return as PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    # In SFT, labels are typically the input_ids themselves\n",
    "    # Shift them inside the model for causal language modeling\n",
    "    return {\n",
    "        \"input_ids\": tokenized_output[\"input_ids\"].squeeze().tolist(), # Convert to list of int\n",
    "        \"attention_mask\": tokenized_output[\"attention_mask\"].squeeze().tolist(), # Convert to list of int\n",
    "        \"labels\": tokenized_output[\"input_ids\"].squeeze().tolist() # Labels are the same as input_ids for causal LM\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_predictions_for_eval(model_to_use, tokenizer_to_use, dataset_to_eval):\n",
    "    predictions_list = []\n",
    "    ground_truths_list = []\n",
    "    \n",
    "    print(f\"\\nGenerating predictions for {len(dataset_to_eval)} validation samples...\")\n",
    "    for example in tqdm(dataset_to_eval, desc=\"Validation Prediction\"):\n",
    "        # When evaluating, 'example' still comes from the raw dataset\n",
    "        comment_text = example['content']\n",
    "        ground_truth_output = example['output']\n",
    "\n",
    "        user_message_content = f\"{OPTIMIZED_PROMPT_CORE_INSTRUCTIONS}\\n\\n{FEW_SHOT_EXAMPLES_TEXT}\\n\\n[待处理文本]\\n{comment_text}\"\n",
    "        \n",
    "        prompt_for_model = tokenizer_to_use.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": OPTIMIZED_PROMPT_SYSTEM_MESSAGE},\n",
    "                {\"role\": \"user\", \"content\": user_message_content}\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer_to_use(prompt_for_model, return_tensors=\"pt\", padding=False, truncation=True, max_length=config.MAX_SEQ_LENGTH).to(model_to_use.device)\n",
    "        input_ids_len = inputs.input_ids.shape[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_to_use.generate(\n",
    "                input_ids=inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=512, \n",
    "                pad_token_id=tokenizer_to_use.pad_token_id if tokenizer_to_use.pad_token_id is not None else tokenizer_to_use.eos_token_id,\n",
    "                eos_token_id=tokenizer_to_use.eos_token_id,\n",
    "                do_sample=True, temperature=0.6, top_p=0.9,\n",
    "            )\n",
    "        \n",
    "        generated_ids = outputs[0][input_ids_len:]\n",
    "        assistant_response = tokenizer_to_use.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        if not assistant_response:\n",
    "            assistant_response = \"NULL | NULL | non-hate | non-hate [END]\"\n",
    "        elif not assistant_response.strip().endswith(\"[END]\"):\n",
    "            assistant_response = assistant_response.strip() + \" [END]\"\n",
    "\n",
    "        predictions_list.append(assistant_response)\n",
    "        ground_truths_list.append(ground_truth_output)\n",
    "        \n",
    "    return predictions_list, ground_truths_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Global Tokenizer Initialization ---\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL_ID, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        # tokenizer.pad_token_id = tokenizer.eos_token_id # SFTTrainer handles this\n",
    "\n",
    "\n",
    "    # --- Load and Split Dataset ---\n",
    "    try:\n",
    "        full_dataset = load_dataset('json', data_files=config.TRAIN_FILE, split=\"train\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading training data from {config.TRAIN_FILE}: {e}\")\n",
    "        exit()\n",
    "\n",
    "    train_dataset_split = None\n",
    "    eval_dataset_split = None\n",
    "\n",
    "    if config.VALIDATION_SPLIT_RATIO > 0 and config.VALIDATION_SPLIT_RATIO < 1:\n",
    "        print(f\"Splitting dataset with ratio: {config.VALIDATION_SPLIT_RATIO}\")\n",
    "        if len(full_dataset) == 0:\n",
    "            print(\"Error: Training dataset is empty. Cannot split.\")\n",
    "            exit()\n",
    "        \n",
    "        # Calculate minimum number of samples for validation\n",
    "        min_validation_samples = 1 \n",
    "        \n",
    "        # Check if there are enough samples for a meaningful split\n",
    "        if len(full_dataset) > min_validation_samples and (len(full_dataset) * config.VALIDATION_SPLIT_RATIO) >= min_validation_samples:\n",
    "            split_dataset = full_dataset.train_test_split(test_size=config.VALIDATION_SPLIT_RATIO, shuffle=True, seed=42)\n",
    "            train_dataset_split = split_dataset['train']\n",
    "            eval_dataset_split = split_dataset['test']\n",
    "            print(f\"Training samples: {len(train_dataset_split)}, Validation samples: {len(eval_dataset_split)}\")\n",
    "        else:\n",
    "            print(f\"Warning: Dataset too small for specified validation split ratio ({config.VALIDATION_SPLIT_RATIO}). Using full dataset for training without SFT validation.\")\n",
    "            train_dataset_split = full_dataset\n",
    "            eval_dataset_split = None\n",
    "    else:\n",
    "        train_dataset_split = full_dataset\n",
    "        eval_dataset_split = None\n",
    "        print(f\"Using full dataset for training: {len(train_dataset_split)} samples. No validation split during SFT.\")\n",
    "\n",
    "\n",
    "    # --- Pre-process datasets with formatting_func to add 'input_ids' and 'labels' columns ---\n",
    "    print(\"Applying formatting_func to training dataset...\")\n",
    "    # `remove_columns` should remove original columns like 'content', 'output', 'id'\n",
    "    # We only want 'input_ids', 'attention_mask', 'labels'\n",
    "    train_dataset_processed = train_dataset_split.map(\n",
    "        sft_formatting_func, \n",
    "        remove_columns=train_dataset_split.column_names, # Remove all original columns\n",
    "        # num_proc=os.cpu_count() or 1, # Uncomment and adjust num_proc if you have enough CPU cores\n",
    "        desc=\"Formatting train dataset\"\n",
    "    )\n",
    "\n",
    "    eval_dataset_processed = None\n",
    "    if eval_dataset_split and len(eval_dataset_split) > 0:\n",
    "        print(\"Applying formatting_func to evaluation dataset...\")\n",
    "        eval_dataset_processed = eval_dataset_split.map(\n",
    "            sft_formatting_func, \n",
    "            remove_columns=eval_dataset_split.column_names, # Remove all original columns\n",
    "            # num_proc=os.cpu_count() or 1, # Uncomment and adjust num_proc if you have enough CPU cores\n",
    "            desc=\"Formatting eval dataset\"\n",
    "        )\n",
    "    # else: eval_dataset_processed remains None\n",
    "\n",
    "    # --- Model Configuration (QLoRA) ---\n",
    "    compute_dtype = getattr(torch, config.BNB_4BIT_COMPUTE_DTYPE)\n",
    "    bnb_config = None\n",
    "    if config.USE_4BIT_QUANTIZATION:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=config.BNB_4BIT_QUANT_TYPE,\n",
    "            bnb_4bit_compute_dtype=compute_dtype,\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "\n",
    "    # --- Load Base Model ---\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.BASE_MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=compute_dtype if not config.USE_4BIT_QUANTIZATION else None,\n",
    "        device_map={\"\": 0}, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.config.use_cache = False \n",
    "    if hasattr(model.config, \"pretraining_tp\"): model.config.pretraining_tp = 1\n",
    "\n",
    "    # --- PEFT Configuration ---\n",
    "    peft_config = LoraConfig(\n",
    "        r=config.LORA_R,\n",
    "        lora_alpha=config.LORA_ALPHA,\n",
    "        lora_dropout=config.LORA_DROPOUT,\n",
    "        target_modules=config.LORA_TARGET_MODULES,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    if config.USE_4BIT_QUANTIZATION:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    if not isinstance(model, PeftModel): \n",
    "        model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # --- Training Arguments ---\n",
    "    if len(train_dataset_split) == 0: # Use original split for total samples calculation\n",
    "        print(\"Error: Training dataset is empty after splitting (or was empty initially). Cannot proceed.\")\n",
    "        exit()\n",
    "\n",
    "    total_steps_approx = (len(train_dataset_split) // (config.TRAIN_BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS)) * config.NUM_TRAIN_EPOCHS\n",
    "    actual_save_steps = int(total_steps_approx * config.SAVE_STEPS_RATIO) if config.SAVE_STEPS_RATIO > 0 and total_steps_approx > 0 else config.LOGGING_STEPS * 5\n",
    "    if actual_save_steps == 0: actual_save_steps = max(1, config.LOGGING_STEPS) \n",
    "\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=config.OUTPUT_DIR,\n",
    "        per_device_train_batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        optim=\"paged_adamw_8bit\" if config.USE_4BIT_QUANTIZATION else \"adamw_torch\",\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        num_train_epochs=config.NUM_TRAIN_EPOCHS,\n",
    "        logging_steps=config.LOGGING_STEPS,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=actual_save_steps,\n",
    "        save_total_limit=2,\n",
    "        fp16=False, \n",
    "        bf16=True if config.BNB_4BIT_COMPUTE_DTYPE == \"bfloat16\" and torch.cuda.is_bf16_supported() else False,\n",
    "        evaluation_strategy=\"steps\" if eval_dataset_split and len(eval_dataset_split) > 0 else \"no\",\n",
    "        eval_steps=actual_save_steps if eval_dataset_split and len(eval_dataset_split) > 0 else None,\n",
    "        report_to=\"tensorboard\", \n",
    "        remove_unused_columns=False, # Now that we explicitly map, keep this False\n",
    "    )\n",
    "\n",
    "    # --- Initialize SFTTrainer ---\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        train_dataset=train_dataset_processed, # Pass the pre-processed dataset with 'input_ids' and 'labels'\n",
    "        eval_dataset=eval_dataset_processed if eval_dataset_processed and len(eval_dataset_processed) > 0 else None, \n",
    "        peft_config=peft_config if isinstance(model, PeftModel) else None, \n",
    "        # Removed formatting_func: it's now applied during map\n",
    "        # Removed dataset_text_field: no longer needed as formatting_func handles tokenization\n",
    "        max_seq_length=config.MAX_SEQ_LENGTH, # Still relevant for data collator's padding/truncation\n",
    "        packing=False, # Make sure packing is False if you are padding to max_length\n",
    "        callbacks=[TqdmProgressCallback()]\n",
    "    )\n",
    "\n",
    "    # --- Train ---\n",
    "    print(\"\\nStarting SFT training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # --- Save Final Model (LoRA adapter) ---\n",
    "    final_adapter_dir = os.path.join(config.OUTPUT_DIR, \"final_lora_adapter\")\n",
    "    trainer.model.save_pretrained(final_adapter_dir)\n",
    "    tokenizer.save_pretrained(final_adapter_dir) \n",
    "    print(f\"\\nFinal LoRA adapter and tokenizer saved to: {final_adapter_dir}\")\n",
    "\n",
    "    # --- Plot Training History ---\n",
    "    if trainer.state.log_history:\n",
    "        plot_training_history(\n",
    "            log_history=trainer.state.log_history,\n",
    "            output_dir=config.TRAINING_PLOTS_DIR,\n",
    "            train_dataset_size=len(train_dataset_split), # Use original train dataset size for steps calculation\n",
    "            train_batch_size=config.TRAIN_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS\n",
    "        )\n",
    "    else:\n",
    "        print(\"No log history found to plot training loss.\")\n",
    "\n",
    "    # --- Evaluate on Validation Set (if it exists) and Plot Metrics ---\n",
    "    # For evaluation, we still need 'content' and 'output' from the ORIGINAL dataset.\n",
    "    # So we use eval_dataset_split, not eval_dataset_processed.\n",
    "    if eval_dataset_split and len(eval_dataset_split) > 0:\n",
    "        print(\"\\n--- Evaluating on Validation Split Post-Training ---\")\n",
    "        validation_predictions, validation_ground_truths = generate_predictions_for_eval(\n",
    "            trainer.model, tokenizer, eval_dataset_split # Pass original eval_dataset_split\n",
    "        )\n",
    "        \n",
    "        os.makedirs(os.path.dirname(config.VALIDATION_PREDICTIONS_FILE), exist_ok=True)\n",
    "        with open(config.VALIDATION_PREDICTIONS_FILE, 'w', encoding='utf-8') as f_val_pred:\n",
    "            for pred_str in validation_predictions:\n",
    "                f_val_pred.write(pred_str + '\\n')\n",
    "        print(f\"Validation split predictions saved to {config.VALIDATION_PREDICTIONS_FILE}\")\n",
    "\n",
    "        parsed_pred_quads_lists = [parse_output_line(line) for line in validation_predictions]\n",
    "        parsed_gt_quads_lists = [parse_output_line(line) for line in validation_ground_truths]\n",
    "\n",
    "        val_results = calculate_f1_metrics_from_lists(parsed_pred_quads_lists, parsed_gt_quads_lists)\n",
    "        \n",
    "        if val_results:\n",
    "            print(\"\\nValidation Set Evaluation Results:\")\n",
    "            print(f\"  Hard Match -> F1: {val_results['hard_match']['f1']:.4f}, P: {val_results['hard_match']['precision']:.4f}, R: {val_results['hard_match']['recall']:.4f}\")\n",
    "            print(f\"  Soft Match -> F1: {val_results['soft_match']['f1']:.4f}, P: {val_results['soft_match']['precision']:.4f}, R: {val_results['soft_match']['recall']:.4f}\")\n",
    "            print(f\"  Overall Score (Avg F1): {val_results['overall_score']:.4f}\")\n",
    "            \n",
    "            plot_evaluation_scores(val_results, config.VALIDATION_EVAL_PLOTS_DIR, \"validation_set_f1_scores.png\")\n",
    "        else:\n",
    "            print(\"Validation evaluation could not be completed.\")\n",
    "    else:\n",
    "        print(\"\\nNo validation set was used or it was empty; skipping post-training validation evaluation.\")\n",
    "\n",
    "    print(\"\\nTraining and validation evaluation (if applicable) complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f3669c9-84d4-4126-9138-634423708cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f5821-6385-4f43-8c33-0567afcc98d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
